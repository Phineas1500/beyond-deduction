{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Phase 2a: Training Interpretable Transformer (Multi-Hop Reasoning Test)\n\nThis notebook trains a small transformer on **multi-hop inductive reasoning** tasks.\n\n**Experimental Design**:\n- **depth-2, depth_of_truth=0**: Model must traverse **1 hop** up the ontology tree\n- **depth-3, depth_of_truth=0**: Model must traverse **2 hops** up the ontology tree\n\nBy filtering both depths to `depth_of_truth=0` (root-level targets), we get a fair comparison that directly tests whether transformers can compose multiple inference steps.\n\n**Research Question**: \nCan transformers perform multi-hop reasoning? If 1-hop succeeds but 2-hop fails, this suggests an architectural limit on compositional reasoning.\n\n**Training Setup**:\n1. Set `TREE_DEPTH = 2` or `TREE_DEPTH = 3` in the configuration cell\n2. Run all cells in order\n3. Training takes ~30-60 min on Colab GPU\n4. Download the trained model at the end\n\n**Data Format (True Induction)**:\n- Observations include both concept membership AND property assertions\n- Model must induce the hidden rule from observations (not pattern completion)\n- All samples require finding the ROOT concept (Occam's Razor / parsimony)"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: command not found: nvidia-smi\n"
     ]
    }
   ],
   "source": [
    "# Check GPU\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 1: All Imports and Class Definitions\n",
    "\n",
    "This cell defines the Tokenizer, Dataset, and Model classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "#@title Run this cell first - Defines all classes { display-mode: \"form\" }\n\nimport json\nimport math\nimport random\nfrom typing import List, Dict, Optional, Tuple, Any, Set\nfrom dataclasses import dataclass, field\nfrom collections import deque, defaultdict\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.optim import AdamW\nfrom torch.optim.lr_scheduler import CosineAnnealingLR\n\nimport matplotlib.pyplot as plt\nfrom IPython.display import clear_output\nimport re\n\n# ============================================================================\n# TOKENIZER (with configurable vocabulary size for deeper trees)\n# ============================================================================\n\nclass SymbolicOntologyTokenizer:\n    \"\"\"Tokenizer for symbolic ontology notation.\"\"\"\n    \n    PAD_TOKEN = \"<PAD>\"\n    BOS_TOKEN = \"<BOS>\"\n    EOS_TOKEN = \"<EOS>\"\n    UNK_TOKEN = \"<UNK>\"\n    WORLD_MODEL_TOKEN = \"[WORLD_MODEL]\"\n    OBSERVATIONS_TOKEN = \"[OBSERVATIONS]\"\n    TASK_TOKEN = \"[TASK]\"\n    ANSWER_TOKEN = \"[ANSWER]\"\n    FORALL_TOKEN = \"‚àÄx:\"\n    IMPLIES_TOKEN = \"->\"\n    OPEN_PAREN = \"(\"\n    CLOSE_PAREN = \")\"\n    PRED_X = \"(x)\"\n    \n    def __init__(self, max_concepts=30, max_properties=15, max_entities=30, max_seq_len=512):\n        self.max_concepts = max_concepts\n        self.max_properties = max_properties\n        self.max_entities = max_entities\n        self.max_seq_len = max_seq_len\n        self._build_vocab()\n        \n    def _build_vocab(self):\n        self.token_to_id = {}\n        self.id_to_token = {}\n        current_id = 0\n        \n        for token in [self.PAD_TOKEN, self.BOS_TOKEN, self.EOS_TOKEN, self.UNK_TOKEN]:\n            self.token_to_id[token] = current_id\n            self.id_to_token[current_id] = token\n            current_id += 1\n        \n        for token in [self.WORLD_MODEL_TOKEN, self.OBSERVATIONS_TOKEN, \n                      self.TASK_TOKEN, self.ANSWER_TOKEN]:\n            self.token_to_id[token] = current_id\n            self.id_to_token[current_id] = token\n            current_id += 1\n        \n        for token in [self.FORALL_TOKEN, self.IMPLIES_TOKEN, \n                      self.OPEN_PAREN, self.CLOSE_PAREN, self.PRED_X]:\n            self.token_to_id[token] = current_id\n            self.id_to_token[current_id] = token\n            current_id += 1\n        \n        self.token_to_id[\"\\n\"] = current_id\n        self.id_to_token[current_id] = \"\\n\"\n        current_id += 1\n        \n        for i in range(self.max_concepts):\n            token = f\"c{i}\"\n            self.token_to_id[token] = current_id\n            self.id_to_token[current_id] = token\n            current_id += 1\n        \n        for i in range(1, self.max_properties + 1):\n            token = f\"p{i}\"\n            self.token_to_id[token] = current_id\n            self.id_to_token[current_id] = token\n            current_id += 1\n        \n        for i in range(self.max_entities):\n            token = f\"e{i}\"\n            self.token_to_id[token] = current_id\n            self.id_to_token[current_id] = token\n            current_id += 1\n        \n        self.vocab_size = current_id\n        self.pad_token_id = self.token_to_id[self.PAD_TOKEN]\n        self.bos_token_id = self.token_to_id[self.BOS_TOKEN]\n        self.eos_token_id = self.token_to_id[self.EOS_TOKEN]\n        self.unk_token_id = self.token_to_id[self.UNK_TOKEN]\n    \n    def _tokenize_statement(self, statement):\n        statement = statement.strip()\n        tokens = []\n        \n        if statement.startswith(\"‚àÄx:\"):\n            tokens.append(self.FORALL_TOKEN)\n            statement = statement[3:].strip()\n        \n        if \"->\" in statement:\n            parts = statement.split(\"->\")\n            left = parts[0].strip()\n            left_match = re.match(r'([cp]\\d+)\\(x\\)', left)\n            if left_match:\n                tokens.append(left_match.group(1))\n                tokens.append(self.PRED_X)\n            tokens.append(self.IMPLIES_TOKEN)\n            right = parts[1].strip()\n            right_match = re.match(r'([cp]\\d+)\\(x\\)', right)\n            if right_match:\n                tokens.append(right_match.group(1))\n                tokens.append(self.PRED_X)\n        else:\n            match = re.match(r'([cp]\\d+)\\(([e]\\d+)\\)', statement)\n            if match:\n                tokens.append(match.group(1))\n                tokens.append(self.OPEN_PAREN)\n                tokens.append(match.group(2))\n                tokens.append(self.CLOSE_PAREN)\n        return tokens\n    \n    def tokenize(self, text, add_special_tokens=True):\n        tokens = []\n        if add_special_tokens:\n            tokens.append(self.BOS_TOKEN)\n        \n        lines = text.strip().split('\\n')\n        for line in lines:\n            line = line.strip()\n            if not line:\n                continue\n            if line == \"[WORLD_MODEL]\":\n                tokens.append(self.WORLD_MODEL_TOKEN)\n                tokens.append(\"\\n\")\n            elif line == \"[OBSERVATIONS]\":\n                tokens.append(self.OBSERVATIONS_TOKEN)\n                tokens.append(\"\\n\")\n            elif line == \"[TASK]\":\n                tokens.append(self.TASK_TOKEN)\n                tokens.append(\"\\n\")\n            elif line == \"[ANSWER]\":\n                tokens.append(self.ANSWER_TOKEN)\n                tokens.append(\"\\n\")\n            elif line.startswith(\"Infer\"):\n                continue  # Skip task description text\n            else:\n                stmt_tokens = self._tokenize_statement(line)\n                tokens.extend(stmt_tokens)\n                tokens.append(\"\\n\")\n        \n        if add_special_tokens:\n            tokens.append(self.EOS_TOKEN)\n        return tokens\n    \n    def encode(self, text, add_special_tokens=True):\n        tokens = self.tokenize(text, add_special_tokens)\n        return [self.token_to_id.get(t, self.unk_token_id) for t in tokens]\n    \n    def decode(self, token_ids, skip_special_tokens=True):\n        special_ids = {self.pad_token_id, self.bos_token_id, self.eos_token_id}\n        tokens = []\n        for tid in token_ids:\n            if skip_special_tokens and tid in special_ids:\n                continue\n            tokens.append(self.id_to_token.get(tid, self.UNK_TOKEN))\n        return \"\".join(tokens)\n\n# ============================================================================\n# DATA GENERATOR (True Induction Format - from generate_symbolic_ontology.py)\n# ============================================================================\n\n@dataclass\nclass OntologyNode:\n    \"\"\"Represents a concept node in the ontology tree.\"\"\"\n    concept_id: int\n    depth: int\n    parent_id: Optional[int] = None\n    children_ids: List[int] = field(default_factory=list)\n    properties: List[int] = field(default_factory=list)\n    members: List[int] = field(default_factory=list)\n\n\nclass SymbolicOntologyGenerator:\n    \"\"\"\n    Generates symbolic ontology trees with controlled complexity for MI research.\n    \n    This is the TRUE INDUCTION version that:\n    1. Includes concept membership AND property observations\n    2. Forces model to induce rules from scratch (not pattern completion)\n    3. Tests Occam's Razor / parsimony reasoning\n    \"\"\"\n    \n    def __init__(self, depth: int = 3, branching_factor: int = 2, \n                 num_properties: int = 5, property_assignment_prob: float = 0.4,\n                 members_per_leaf: int = 2, seed: Optional[int] = None):\n        self.depth = depth\n        self.branching_factor = branching_factor\n        self.num_properties = num_properties\n        self.property_assignment_prob = property_assignment_prob\n        self.members_per_leaf = members_per_leaf\n        \n        if seed is not None:\n            random.seed(seed)\n        \n        self.nodes: Dict[int, OntologyNode] = {}\n        self.num_concepts = 0\n        self.num_members = 0\n        self._generate_structure()\n    \n    def _generate_structure(self):\n        \"\"\"Build the ontology tree with BFS traversal.\"\"\"\n        root = OntologyNode(concept_id=0, depth=0)\n        self.nodes[0] = root\n        self.num_concepts = 1\n        \n        queue = deque([0])\n        \n        while queue:\n            curr_id = queue.popleft()\n            curr_node = self.nodes[curr_id]\n            \n            # Assign properties randomly (distributed throughout tree)\n            if random.random() < self.property_assignment_prob:\n                used_props = self._get_ancestor_properties(curr_id)\n                available_props = [p for p in range(1, self.num_properties + 1) \n                                   if p not in used_props]\n                if available_props:\n                    prop_id = random.choice(available_props)\n                    curr_node.properties.append(prop_id)\n            \n            # Add children if not at max depth\n            if curr_node.depth < self.depth - 1:\n                for _ in range(self.branching_factor):\n                    child_id = self.num_concepts\n                    self.num_concepts += 1\n                    \n                    child = OntologyNode(\n                        concept_id=child_id,\n                        depth=curr_node.depth + 1,\n                        parent_id=curr_id\n                    )\n                    curr_node.children_ids.append(child_id)\n                    self.nodes[child_id] = child\n                    queue.append(child_id)\n            else:\n                # Leaf node: assign members (entities)\n                for _ in range(self.members_per_leaf):\n                    member_id = self.num_members\n                    curr_node.members.append(member_id)\n                    self.num_members += 1\n    \n    def _get_ancestor_properties(self, concept_id: int) -> Set[int]:\n        \"\"\"Get all properties from ancestors to avoid conflicts.\"\"\"\n        props = set()\n        curr_id = concept_id\n        while curr_id is not None:\n            node = self.nodes[curr_id]\n            props.update(node.properties)\n            curr_id = node.parent_id\n        return props\n    \n    def _get_all_descendants(self, concept_id: int) -> List[int]:\n        \"\"\"Get all descendant concept IDs (including self).\"\"\"\n        descendants = [concept_id]\n        queue = deque([concept_id])\n        while queue:\n            curr = queue.popleft()\n            for child_id in self.nodes[curr].children_ids:\n                descendants.append(child_id)\n                queue.append(child_id)\n        return descendants\n    \n    def _get_all_descendant_members(self, concept_id: int) -> List[Tuple[int, int]]:\n        \"\"\"Get all entity members that belong to a concept (including from subtypes).\"\"\"\n        members = []\n        descendants = self._get_all_descendants(concept_id)\n        for desc_id in descendants:\n            for member_id in self.nodes[desc_id].members:\n                members.append((member_id, desc_id))\n        return members\n\n\ndef generate_inductive_sample(gen: SymbolicOntologyGenerator) -> Optional[Dict]:\n    \"\"\"\n    Generate a TRUE INDUCTIVE reasoning task.\n    \n    Task: Given observations that multiple entities have a property,\n    infer the most general rule (find the common ancestor).\n    \n    Key difference from pattern-completion:\n    - Observations include BOTH concept membership AND property assertions\n    - Model must induce the rule from scratch\n    \"\"\"\n    # Find concepts with properties that have descendants with members\n    candidates = []\n    for concept_id, node in gen.nodes.items():\n        if node.properties:\n            descendant_members = gen._get_all_descendant_members(concept_id)\n            if len(descendant_members) >= 2:\n                for prop_id in node.properties:\n                    candidates.append((concept_id, prop_id, descendant_members))\n    \n    if not candidates:\n        return None\n    \n    target_c, target_p, all_members = random.choice(candidates)\n    target_node = gen.nodes[target_c]\n    \n    # Select entities from different subtrees for parsimony testing\n    num_observations = min(3, len(all_members))\n    if len(all_members) <= num_observations:\n        selected = all_members\n    else:\n        selected = random.sample(all_members, num_observations)\n    \n    # Generate observations (TRUE INDUCTION FORMAT)\n    # Include BOTH concept membership AND property observation\n    observations = []\n    for entity_id, leaf_id in selected:\n        observations.append(f\"c{leaf_id}(e{entity_id})\")  # Concept membership\n        observations.append(f\"p{target_p}(e{entity_id})\")  # Property observation\n    \n    # Ground truth hypothesis (parsimonious)\n    gt_hypothesis = f\"‚àÄx: c{target_c}(x) -> p{target_p}(x)\"\n    \n    # Build world model (without the hidden hypothesis)\n    world_model = []\n    for node in gen.nodes.values():\n        # Subtype relations\n        if node.parent_id is not None:\n            world_model.append(f\"‚àÄx: c{node.concept_id}(x) -> c{node.parent_id}(x)\")\n        \n        # Property rules (EXCEPT the hidden target)\n        for prop_id in node.properties:\n            if not (node.concept_id == target_c and prop_id == target_p):\n                world_model.append(f\"‚àÄx: c{node.concept_id}(x) -> p{prop_id}(x)\")\n        \n        # Membership\n        for member_id in node.members:\n            world_model.append(f\"c{node.concept_id}(e{member_id})\")\n    \n    random.shuffle(world_model)\n    random.shuffle(observations)\n    \n    # Format for training\n    prompt = \"[WORLD_MODEL]\\n\" + \"\\n\".join(world_model)\n    prompt += \"\\n[OBSERVATIONS]\\n\" + \"\\n\".join(observations)\n    prompt += \"\\n[TASK]\\nInfer the most general rule that explains all observations.\"\n    \n    return {\n        \"input\": prompt,\n        \"target\": gt_hypothesis,\n        \"task_type\": \"inductive\",\n        \"metadata\": {\n            \"target_concept\": f\"c{target_c}\",\n            \"target_property\": f\"p{target_p}\",\n            \"depth_of_truth\": target_node.depth,\n            \"tree_depth\": gen.depth,\n            \"branching_factor\": gen.branching_factor,\n            \"num_observations\": len(selected),\n            \"is_parsimony_test\": len(set(leaf for _, leaf in selected)) > 1,\n            \"observed_entities\": [f\"e{eid}\" for eid, _ in selected],\n            \"observed_leaf_concepts\": list(set(f\"c{lid}\" for _, lid in selected))\n        }\n    }\n\n\ndef generate_dataset(num_samples: int, min_depth: int = 2, max_depth: int = 4,\n                     min_branch: int = 2, max_branch: int = 3, seed: int = 42) -> List[Dict]:\n    \"\"\"Generate a dataset of inductive reasoning examples.\"\"\"\n    random.seed(seed)\n    samples = []\n    attempts = 0\n    max_attempts = num_samples * 10\n    \n    while len(samples) < num_samples and attempts < max_attempts:\n        attempts += 1\n        \n        depth = random.randint(min_depth, max_depth)\n        branch = random.randint(min_branch, max_branch)\n        \n        gen = SymbolicOntologyGenerator(\n            depth=depth,\n            branching_factor=branch,\n            num_properties=5,\n            property_assignment_prob=0.4\n        )\n        \n        sample = generate_inductive_sample(gen)\n        if sample:\n            sample[\"id\"] = len(samples)\n            samples.append(sample)\n    \n    return samples\n\n# ============================================================================\n# DATASET CLASS\n# ============================================================================\n\nclass SymbolicOntologyDataset(Dataset):\n    def __init__(self, samples, tokenizer, max_input_len=512, max_target_len=32):\n        self.samples = samples\n        self.tokenizer = tokenizer\n        self.max_input_len = max_input_len\n        self.max_target_len = max_target_len\n    \n    def __len__(self):\n        return len(self.samples)\n    \n    def __getitem__(self, idx):\n        sample = self.samples[idx]\n        input_ids = self.tokenizer.encode(sample[\"input\"], add_special_tokens=True)\n        target_ids = self.tokenizer.encode(sample[\"target\"], add_special_tokens=False)\n        target_ids = target_ids + [self.tokenizer.eos_token_id]\n        \n        answer_token_id = self.tokenizer.token_to_id[self.tokenizer.ANSWER_TOKEN]\n        newline_id = self.tokenizer.token_to_id[\"\\n\"]\n        \n        if input_ids[-1] == self.tokenizer.eos_token_id:\n            input_ids = input_ids[:-1]\n        \n        max_input_tokens = self.max_input_len - 2\n        if len(input_ids) > max_input_tokens:\n            input_ids = input_ids[:max_input_tokens]\n        if len(target_ids) > self.max_target_len:\n            target_ids = target_ids[:self.max_target_len]\n        \n        full_ids = input_ids + [answer_token_id, newline_id] + target_ids\n        max_len = self.max_input_len + self.max_target_len\n        \n        if len(full_ids) > max_len:\n            full_ids = full_ids[:max_len]\n        \n        target_start = len(input_ids) + 2\n        attention_mask = [1] * len(full_ids)\n        padding_len = max_len - len(full_ids)\n        full_ids = full_ids + [self.tokenizer.pad_token_id] * padding_len\n        attention_mask = attention_mask + [0] * padding_len\n        \n        labels = full_ids.copy()\n        labels[:target_start] = [-100] * target_start\n        \n        return {\n            \"input_ids\": torch.tensor(full_ids, dtype=torch.long),\n            \"labels\": torch.tensor(labels, dtype=torch.long),\n            \"attention_mask\": torch.tensor(attention_mask, dtype=torch.long),\n        }\n\n# ============================================================================\n# MODEL\n# ============================================================================\n\n@dataclass\nclass TransformerConfig:\n    vocab_size: int = 100\n    max_seq_len: int = 512\n    n_layers: int = 4\n    n_heads: int = 1\n    d_model: int = 64\n    d_ff: int = 256\n    dropout: float = 0.0\n    pad_token_id: int = 0\n    concat_pos_emb: bool = True\n    pre_ln: bool = True\n    causal: bool = True\n    use_ff: bool = True\n    \n    @property\n    def embedding_dim(self):\n        if self.concat_pos_emb:\n            return self.d_model + self.max_seq_len\n        return self.d_model\n\nclass SingleHeadAttention(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.config = config\n        input_dim = config.embedding_dim\n        self.proj_q = nn.Linear(input_dim, config.d_model, bias=True)\n        self.proj_k = nn.Linear(input_dim, config.d_model, bias=True)\n        self.proj_v = nn.Linear(input_dim, config.d_model, bias=True)\n        self.proj_out = nn.Linear(config.d_model, input_dim, bias=True)\n        self.dropout = nn.Dropout(config.dropout)\n        self.scale = math.sqrt(config.d_model)\n        \n    def forward(self, x, attention_mask=None, return_attention=False):\n        batch_size, seq_len, _ = x.shape\n        Q = self.proj_q(x)\n        K = self.proj_k(x)\n        V = self.proj_v(x)\n        scores = torch.matmul(Q, K.transpose(-2, -1)) / self.scale\n        \n        if self.config.causal:\n            causal_mask = torch.triu(torch.ones(seq_len, seq_len, device=x.device, dtype=torch.bool), diagonal=1)\n            scores = scores.masked_fill(causal_mask, float('-inf'))\n        \n        if attention_mask is not None:\n            padding_mask = (attention_mask == 0).unsqueeze(1)\n            scores = scores.masked_fill(padding_mask, float('-inf'))\n        \n        attention_weights = F.softmax(scores, dim=-1)\n        attention_weights = self.dropout(attention_weights)\n        output = torch.matmul(attention_weights, V)\n        output = self.proj_out(output)\n        \n        if return_attention:\n            return output, attention_weights\n        return output, None\n\nclass FeedForward(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        input_dim = config.embedding_dim\n        self.fc1 = nn.Linear(input_dim, config.d_ff)\n        self.fc2 = nn.Linear(config.d_ff, input_dim)\n        self.dropout = nn.Dropout(config.dropout)\n        \n    def forward(self, x):\n        x = self.fc1(x)\n        x = F.relu(x)\n        x = self.dropout(x)\n        x = self.fc2(x)\n        return x\n\nclass TransformerBlock(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.config = config\n        self.ln_attn = nn.LayerNorm(config.embedding_dim)\n        self.attn = SingleHeadAttention(config)\n        if config.use_ff:\n            self.ln_ff = nn.LayerNorm(config.embedding_dim)\n            self.ff = FeedForward(config)\n        else:\n            self.ln_ff = None\n            self.ff = None\n        self.dropout = nn.Dropout(config.dropout)\n        \n    def forward(self, x, attention_mask=None, return_attention=False):\n        if self.config.pre_ln:\n            attn_out, attn_weights = self.attn(self.ln_attn(x), attention_mask, return_attention)\n            x = x + self.dropout(attn_out)\n            if self.ff is not None:\n                x = x + self.dropout(self.ff(self.ln_ff(x)))\n        else:\n            attn_out, attn_weights = self.attn(x, attention_mask, return_attention)\n            x = self.ln_attn(x + self.dropout(attn_out))\n            if self.ff is not None:\n                x = self.ln_ff(x + self.dropout(self.ff(x)))\n        return x, attn_weights\n\nclass InterpretableTransformer(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.config = config\n        self.token_embedding = nn.Embedding(config.vocab_size, config.d_model, padding_idx=config.pad_token_id)\n        \n        if config.concat_pos_emb:\n            self.register_buffer('position_embedding', torch.eye(config.max_seq_len))\n        else:\n            self.position_embedding = nn.Embedding(config.max_seq_len, config.d_model)\n        \n        self.dropout_embedding = nn.Dropout(config.dropout)\n        self.layers = nn.ModuleList([TransformerBlock(config) for _ in range(config.n_layers)])\n        self.ln_final = nn.LayerNorm(config.embedding_dim)\n        self.lm_head = nn.Linear(config.embedding_dim, config.vocab_size, bias=False)\n        self._init_weights()\n        \n    def _init_weights(self):\n        for module in self.modules():\n            if isinstance(module, nn.Linear):\n                nn.init.normal_(module.weight, mean=0.0, std=0.02)\n                if module.bias is not None:\n                    nn.init.zeros_(module.bias)\n            elif isinstance(module, nn.Embedding):\n                nn.init.normal_(module.weight, mean=0.0, std=0.02)\n                if module.padding_idx is not None:\n                    module.weight.data[module.padding_idx].zero_()\n    \n    def get_embeddings(self, input_ids):\n        batch_size, seq_len = input_ids.shape\n        token_emb = self.token_embedding(input_ids)\n        \n        if self.config.concat_pos_emb:\n            pos_emb = self.position_embedding[:seq_len, :seq_len]\n            pos_emb = pos_emb.unsqueeze(0).expand(batch_size, -1, -1)\n            embeddings = torch.cat([token_emb, pos_emb], dim=-1)\n        else:\n            positions = torch.arange(seq_len, device=input_ids.device)\n            pos_emb = self.position_embedding(positions)\n            embeddings = token_emb + pos_emb\n        \n        return self.dropout_embedding(embeddings)\n    \n    def forward(self, input_ids, attention_mask=None, return_hidden_states=False, return_attention=False):\n        x = self.get_embeddings(input_ids)\n        hidden_states = [x] if return_hidden_states else None\n        attentions = [] if return_attention else None\n        \n        for layer in self.layers:\n            x, attn_weights = layer(x, attention_mask, return_attention)\n            if return_hidden_states:\n                hidden_states.append(x)\n            if return_attention:\n                attentions.append(attn_weights)\n        \n        x = self.ln_final(x)\n        logits = self.lm_head(x)\n        \n        output = {\"logits\": logits}\n        if return_hidden_states:\n            output[\"hidden_states\"] = hidden_states\n        if return_attention:\n            output[\"attentions\"] = attentions\n        return output\n    \n    def generate(self, input_ids, max_new_tokens=32, temperature=1.0, do_sample=False):\n        self.eval()\n        with torch.no_grad():\n            for _ in range(max_new_tokens):\n                outputs = self(input_ids)\n                logits = outputs[\"logits\"][:, -1, :]\n                if do_sample:\n                    probs = F.softmax(logits / temperature, dim=-1)\n                    next_token = torch.multinomial(probs, num_samples=1)\n                else:\n                    next_token = logits.argmax(dim=-1, keepdim=True)\n                input_ids = torch.cat([input_ids, next_token], dim=-1)\n                if (next_token == 2).all():\n                    break\n        return input_ids\n\ndef count_parameters(model):\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n\n\n# ============================================================================\n# HELPER: Calculate required vocab size for a given depth\n# ============================================================================\n\ndef get_vocab_requirements(depth: int, branching: int = 2, members_per_leaf: int = 2):\n    \"\"\"Calculate vocabulary requirements for a given tree depth.\"\"\"\n    # Number of concepts = sum of branching^i for i in 0 to depth-1\n    num_concepts = sum(branching**i for i in range(depth))\n    # Number of leaves = branching^(depth-1)\n    num_leaves = branching ** (depth - 1)\n    # Number of entities = leaves * members_per_leaf\n    num_entities = num_leaves * members_per_leaf\n    return num_concepts, num_entities\n\n# Print requirements for various depths\nprint(\"Vocabulary requirements by depth (branching=2):\")\nprint(\"-\" * 45)\nfor d in range(2, 9):\n    nc, ne = get_vocab_requirements(d, branching=2)\n    print(f\"  Depth {d}: {nc:3d} concepts, {ne:3d} entities\")\nprint(\"-\" * 45)\nprint(\"‚úì All classes defined (TRUE INDUCTION format)!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 2: Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "\n",
      "==================================================\n",
      "TRAINING CONFIGURATION\n",
      "==================================================\n",
      "Tree Depth: 3 (min=3, max=3)\n",
      "Training samples: 2000\n",
      "Model: 4 layers, 1 head(s), d_model=64\n",
      "Output file will be: depth3_trained.pt\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "#@title Training Configuration { display-mode: \"form\" }\n",
    "\n",
    "#@markdown ## ‚ö†Ô∏è IMPORTANT: Set Tree Depth for Training\n",
    "#@markdown Change this value to train different models:\n",
    "#@markdown - `TREE_DEPTH = 2` ‚Üí depth-2 model (should achieve ~98% accuracy)\n",
    "#@markdown - `TREE_DEPTH = 3` ‚Üí depth-3 model (should plateau at ~50% accuracy)\n",
    "\n",
    "TREE_DEPTH = 3  #@param {type:\"integer\"}\n",
    "\n",
    "#@markdown ---\n",
    "#@markdown ### Data Settings\n",
    "NUM_TRAIN_SAMPLES = 2000  #@param {type:\"integer\"}\n",
    "NUM_VAL_SAMPLES = 400  #@param {type:\"integer\"}\n",
    "BRANCHING_FACTOR_MIN = 2  #@param {type:\"integer\"}\n",
    "BRANCHING_FACTOR_MAX = 3  #@param {type:\"integer\"}\n",
    "\n",
    "#@markdown ### Model Architecture\n",
    "N_LAYERS = 4  #@param {type:\"integer\"}\n",
    "N_HEADS = 1  #@param {type:\"integer\"}\n",
    "D_MODEL = 64  #@param {type:\"integer\"}\n",
    "D_FF = 256  #@param {type:\"integer\"}\n",
    "\n",
    "#@markdown ### Training Hyperparameters\n",
    "BATCH_SIZE = 32  #@param {type:\"integer\"}\n",
    "LEARNING_RATE = 1e-3  #@param {type:\"number\"}\n",
    "NUM_EPOCHS = 1000  #@param {type:\"integer\"}\n",
    "GRAD_CLIP = 1.0  #@param {type:\"number\"}\n",
    "\n",
    "#@markdown ### Logging\n",
    "EVAL_INTERVAL = 50  #@param {type:\"integer\"}\n",
    "PLOT_INTERVAL = 100  #@param {type:\"integer\"}\n",
    "\n",
    "# Derived settings\n",
    "MAX_INPUT_LEN = 512\n",
    "MAX_TARGET_LEN = 32\n",
    "MAX_SEQ_LEN = MAX_INPUT_LEN + MAX_TARGET_LEN\n",
    "\n",
    "# Device\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"TRAINING CONFIGURATION\")\n",
    "print(f\"{'='*50}\")\n",
    "print(f\"Tree Depth: {TREE_DEPTH} (min={TREE_DEPTH}, max={TREE_DEPTH})\")\n",
    "print(f\"Training samples: {NUM_TRAIN_SAMPLES}\")\n",
    "print(f\"Model: {N_LAYERS} layers, {N_HEADS} head(s), d_model={D_MODEL}\")\n",
    "print(f\"Output file will be: depth{TREE_DEPTH}_trained.pt\")\n",
    "print(f\"{'='*50}\")\n",
    "\n",
    "# Seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 3: Generate Data and Initialize Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# FAIR COMPARISON: Filter ALL depths to depth_of_truth=0\n# This tests multi-hop reasoning:\n#   - depth-2, dot=0: 1 hop\n#   - depth-3, dot=0: 2 hops\n#   - depth-4, dot=0: 3 hops\n#   - depth-5+: 4+ hops\nFILTER_DEPTH_OF_TRUTH = 0  # Always filter to root-level targets\n\n# Auto-calculate vocabulary requirements\nrequired_concepts, required_entities = get_vocab_requirements(TREE_DEPTH, branching=2)\nprint(f\"Depth-{TREE_DEPTH} requires: {required_concepts} concepts, {required_entities} entities\")\n\n# Set vocabulary sizes with some margin\nVOCAB_CONCEPTS = max(30, required_concepts + 5)\nVOCAB_ENTITIES = max(30, required_entities + 5)\nprint(f\"Using vocabulary: {VOCAB_CONCEPTS} concepts, {VOCAB_ENTITIES} entities\")\n\n# Force branching=2 for all depths (consistent experiment)\nBRANCHING_FACTOR_MIN = 2\nBRANCHING_FACTOR_MAX = 2\nprint(f\"Branching factor fixed at 2 for consistency\")\n\n# Adaptive oversampling - deeper trees have rarer depth_of_truth=0\noversample_factors = {2: 4, 3: 10, 4: 30, 5: 100, 6: 200, 7: 400, 8: 800}\noversample_factor = oversample_factors.get(TREE_DEPTH, 100)\n\nprint(f\"\\nGenerating training data (depth={TREE_DEPTH}, TRUE INDUCTION format)...\")\nprint(f\"Filtering to depth_of_truth={FILTER_DEPTH_OF_TRUTH} for fair comparison\")\nprint(f\"Using oversample_factor={oversample_factor}\")\n\nraw_train = generate_dataset(\n    NUM_TRAIN_SAMPLES * oversample_factor,\n    min_depth=TREE_DEPTH,\n    max_depth=TREE_DEPTH,\n    min_branch=BRANCHING_FACTOR_MIN,\n    max_branch=BRANCHING_FACTOR_MAX,\n    seed=42\n)\n\n# Filter to depth_of_truth=0 only\ntrain_samples = [s for s in raw_train if s['metadata']['depth_of_truth'] == FILTER_DEPTH_OF_TRUTH]\nprint(f\"Filtered: {len(train_samples)} samples with depth_of_truth={FILTER_DEPTH_OF_TRUTH} (from {len(raw_train)} raw)\")\n\n# Check if we have enough samples\nif len(train_samples) < NUM_TRAIN_SAMPLES:\n    print(f\"WARNING: Only got {len(train_samples)} samples, wanted {NUM_TRAIN_SAMPLES}\")\n    print(f\"Generating more samples with different seeds...\")\n    for extra_seed in [999, 1234, 5678, 9999, 11111, 22222, 33333, 44444]:\n        if len(train_samples) >= NUM_TRAIN_SAMPLES:\n            break\n        extra_raw = generate_dataset(\n            NUM_TRAIN_SAMPLES * oversample_factor,\n            min_depth=TREE_DEPTH,\n            max_depth=TREE_DEPTH,\n            min_branch=BRANCHING_FACTOR_MIN,\n            max_branch=BRANCHING_FACTOR_MAX,\n            seed=extra_seed\n        )\n        extra_filtered = [s for s in extra_raw if s['metadata']['depth_of_truth'] == FILTER_DEPTH_OF_TRUTH]\n        train_samples.extend(extra_filtered)\n        print(f\"  Added {len(extra_filtered)} more samples (total: {len(train_samples)})\")\n\nprint(f\"\\nGenerating validation data (depth={TREE_DEPTH})...\")\nraw_val = generate_dataset(\n    NUM_VAL_SAMPLES * oversample_factor,\n    min_depth=TREE_DEPTH,\n    max_depth=TREE_DEPTH,\n    min_branch=BRANCHING_FACTOR_MIN,\n    max_branch=BRANCHING_FACTOR_MAX,\n    seed=123\n)\n\n# Filter to depth_of_truth=0 only\nval_samples = [s for s in raw_val if s['metadata']['depth_of_truth'] == FILTER_DEPTH_OF_TRUTH]\nprint(f\"Filtered: {len(val_samples)} samples with depth_of_truth={FILTER_DEPTH_OF_TRUTH} (from {len(raw_val)} raw)\")\n\n# Show reasoning test info\nhops_required = TREE_DEPTH - 1\nprint(f\"\\n{'='*50}\")\nprint(f\"MULTI-HOP REASONING TEST\")\nprint(f\"{'='*50}\")\nprint(f\"Tree depth: {TREE_DEPTH}\")\nprint(f\"Target depth: {FILTER_DEPTH_OF_TRUTH} (root)\")\nprint(f\"Hops required: {hops_required}\")\nprint(f\"Branching factor: {BRANCHING_FACTOR_MIN}\")\nprint(f\"Training samples: {len(train_samples)}\")\nprint(f\"Validation samples: {len(val_samples)}\")\nprint(f\"{'='*50}\")\n\n# Show a sample\nif train_samples:\n    sample = train_samples[0]\n    print(f\"\\n[SAMPLE INPUT] (first 400 chars):\\n{sample['input'][:400]}...\")\n    print(f\"\\n[TARGET]: {sample['target']}\")\nelse:\n    print(\"\\nERROR: No training samples generated!\")\n    print(\"This depth may be too deep to reliably generate depth_of_truth=0 samples.\")\n\n# Create tokenizer with appropriate vocabulary\ntokenizer = SymbolicOntologyTokenizer(\n    max_concepts=VOCAB_CONCEPTS,\n    max_entities=VOCAB_ENTITIES,\n    max_seq_len=MAX_SEQ_LEN\n)\nprint(f\"\\nVocabulary size: {tokenizer.vocab_size}\")\n\n# Create datasets\ntrain_dataset = SymbolicOntologyDataset(train_samples, tokenizer, MAX_INPUT_LEN, MAX_TARGET_LEN)\nval_dataset = SymbolicOntologyDataset(val_samples, tokenizer, MAX_INPUT_LEN, MAX_TARGET_LEN)\n\n# Create dataloaders\ntrain_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n\nprint(f\"Training batches: {len(train_loader)}\")\nprint(f\"Validation batches: {len(val_loader)}\")\n\n# Create model with matching vocabulary\nmodel_config = TransformerConfig(\n    vocab_size=tokenizer.vocab_size,\n    max_seq_len=MAX_SEQ_LEN,\n    n_layers=N_LAYERS,\n    n_heads=N_HEADS,\n    d_model=D_MODEL,\n    d_ff=D_FF,\n    pad_token_id=tokenizer.pad_token_id,\n)\n\nmodel = InterpretableTransformer(model_config).to(DEVICE)\nn_params = count_parameters(model)\nprint(f\"\\nModel parameters: {n_params:,} ({n_params/1e6:.2f}M)\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Verify filtering worked\nfrom collections import Counter\ntrain_dot = Counter([s['metadata']['depth_of_truth'] for s in train_samples])\nval_dot = Counter([s['metadata']['depth_of_truth'] for s in val_samples])\nprint(f\"Training depth_of_truth distribution: {train_dot}\")\nprint(f\"Validation depth_of_truth distribution: {val_dot}\")\n\n# Verify ALL samples have depth_of_truth=0\nassert all(s['metadata']['depth_of_truth'] == 0 for s in train_samples), \"Filtering failed for train!\"\nassert all(s['metadata']['depth_of_truth'] == 0 for s in val_samples), \"Filtering failed for val!\"\nprint(f\"‚úì All samples have depth_of_truth=0 (root-level targets)\")\n\n# Summary of what we're testing\nprint(f\"\\n{'='*50}\")\nprint(f\"EXPERIMENTAL SETUP SUMMARY\")\nprint(f\"{'='*50}\")\nprint(f\"Tree depth: {TREE_DEPTH}\")\nprint(f\"Hops to traverse: {TREE_DEPTH - 1}\")\nprint(f\"Training samples: {len(train_samples)}\")\nprint(f\"Validation samples: {len(val_samples)}\")\nprint(f\"\\nThis tests whether the model can perform {TREE_DEPTH - 1}-hop\")\nprint(f\"upward traversal in the ontology tree to find the root concept.\")\nprint(f\"{'='*50}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 4: Training Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def compute_loss(model, batch, device):\n    input_ids = batch[\"input_ids\"].to(device)\n    labels = batch[\"labels\"].to(device)\n    attention_mask = batch[\"attention_mask\"].to(device)\n    \n    outputs = model(input_ids, attention_mask=attention_mask)\n    logits = outputs[\"logits\"]\n    \n    shift_logits = logits[:, :-1, :].contiguous().view(-1, logits.size(-1))\n    shift_labels = labels[:, 1:].contiguous().view(-1)\n    \n    return F.cross_entropy(shift_logits, shift_labels, ignore_index=-100)\n\ndef compute_accuracy(model, batch, device):\n    input_ids = batch[\"input_ids\"].to(device)\n    labels = batch[\"labels\"].to(device)\n    attention_mask = batch[\"attention_mask\"].to(device)\n    \n    with torch.no_grad():\n        outputs = model(input_ids, attention_mask=attention_mask)\n        predictions = outputs[\"logits\"].argmax(dim=-1)\n    \n    shift_preds = predictions[:, :-1]\n    shift_labels = labels[:, 1:]\n    target_mask = (shift_labels != -100)\n    \n    correct = ((shift_preds == shift_labels) & target_mask).sum().item()\n    total = target_mask.sum().item()\n    token_acc = correct / total if total > 0 else 0.0\n    \n    seq_correct = []\n    for i in range(shift_labels.size(0)):\n        mask_i = target_mask[i]\n        if mask_i.sum() == 0:\n            seq_correct.append(True)\n        else:\n            seq_correct.append((shift_preds[i][mask_i] == shift_labels[i][mask_i]).all().item())\n    seq_acc = sum(seq_correct) / len(seq_correct)\n    \n    return {\"token_acc\": token_acc, \"seq_acc\": seq_acc}\n\ndef evaluate(model, dataloader, device, max_batches=None):\n    model.eval()\n    total_loss, total_token_acc, total_seq_acc, n = 0, 0, 0, 0\n    \n    with torch.no_grad():\n        for batch in dataloader:\n            loss = compute_loss(model, batch, device)\n            acc = compute_accuracy(model, batch, device)\n            total_loss += loss.item()\n            total_token_acc += acc[\"token_acc\"]\n            total_seq_acc += acc[\"seq_acc\"]\n            n += 1\n            if max_batches and n >= max_batches:\n                break\n    \n    model.train()\n    return {\"loss\": total_loss / n, \"token_acc\": total_token_acc / n, \"seq_acc\": total_seq_acc / n}\n\n# Text-based logger (no inline plots to avoid crashes)\nclass TextLogger:\n    def __init__(self, plot_dir=\".\"):\n        self.history = {'step': [], 'train_loss': [], 'val_loss': [], 'train_seq_acc': [], 'val_seq_acc': []}\n        self.plot_dir = plot_dir\n    \n    def log(self, step, train_metrics, val_metrics=None):\n        self.history['step'].append(step)\n        self.history['train_loss'].append(train_metrics['loss'])\n        self.history['train_seq_acc'].append(train_metrics['seq_acc'])\n        self.history['val_loss'].append(val_metrics['loss'] if val_metrics else None)\n        self.history['val_seq_acc'].append(val_metrics['seq_acc'] if val_metrics else None)\n        \n        # Print text-based stats\n        msg = f\"Step {step:6d} | Train Loss: {train_metrics['loss']:.4f} | Train Acc: {train_metrics['seq_acc']:.1%}\"\n        if val_metrics:\n            msg += f\" | Val Loss: {val_metrics['loss']:.4f} | Val Acc: {val_metrics['seq_acc']:.1%}\"\n            # Status indicator\n            t, v = train_metrics['seq_acc'], val_metrics['seq_acc']\n            if v > 0.9:\n                msg += \" ‚úì\"\n            elif t > 0.9 and v < 0.5:\n                msg += \" (grokking?)\"\n        print(msg)\n    \n    def save_plot(self, filename=None):\n        \"\"\"Save training curves to file instead of displaying inline.\"\"\"\n        if filename is None:\n            filename = f\"{self.plot_dir}/training_curves_depth{TREE_DEPTH}.png\"\n        \n        fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n        steps = self.history['step']\n        \n        # Loss\n        axes[0].plot(steps, self.history['train_loss'], 'b-', label='Train', lw=2)\n        val_loss = [v for v in self.history['val_loss'] if v is not None]\n        if val_loss:\n            val_steps = [s for s, v in zip(steps, self.history['val_loss']) if v is not None]\n            axes[0].plot(val_steps, val_loss, 'r-', label='Val', lw=2)\n        axes[0].set_xlabel('Step'); axes[0].set_ylabel('Loss'); axes[0].set_title('Loss')\n        axes[0].legend(); axes[0].set_yscale('log'); axes[0].grid(True, alpha=0.3)\n        \n        # Sequence Accuracy\n        axes[1].plot(steps, self.history['train_seq_acc'], 'b-', label='Train', lw=2)\n        val_acc = [v for v in self.history['val_seq_acc'] if v is not None]\n        if val_acc:\n            val_steps = [s for s, v in zip(steps, self.history['val_seq_acc']) if v is not None]\n            axes[1].plot(val_steps, val_acc, 'r-', label='Val', lw=2)\n        axes[1].set_xlabel('Step'); axes[1].set_ylabel('Accuracy')\n        axes[1].set_title('Sequence Accuracy')\n        axes[1].legend(); axes[1].set_ylim(-0.05, 1.05); axes[1].grid(True, alpha=0.3)\n        axes[1].axhline(y=0.9, color='g', ls='--', alpha=0.5, label='90%')\n        axes[1].axhline(y=0.5, color='orange', ls='--', alpha=0.5, label='50%')\n        \n        # Gap\n        if val_acc:\n            gaps = [t - v for t, v in zip(self.history['train_seq_acc'], self.history['val_seq_acc']) if v is not None]\n            gap_steps = [s for s, v in zip(steps, self.history['val_seq_acc']) if v is not None]\n            colors = ['green' if g < 0.2 else 'orange' if g < 0.5 else 'red' for g in gaps]\n            axes[2].bar(gap_steps, gaps, color=colors, alpha=0.7, width=max(1, len(gap_steps)//50))\n        axes[2].set_xlabel('Step'); axes[2].set_ylabel('Train - Val')\n        axes[2].set_title('Generalization Gap'); axes[2].grid(True, alpha=0.3)\n        \n        plt.tight_layout()\n        plt.savefig(filename, dpi=150, bbox_inches='tight')\n        plt.close(fig)  # Close to free memory\n        print(f\"Plot saved to {filename}\")\n\nlogger = TextLogger(plot_dir=\".\")\nprint(\"‚úì Training utilities ready (text-based logging, plots saved to files)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 5: Train! üöÄ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Optimizer and scheduler\noptimizer = AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=0.01)\ntotal_steps = len(train_loader) * NUM_EPOCHS\nscheduler = CosineAnnealingLR(optimizer, T_max=total_steps, eta_min=1e-6)\n\n# How often to save plot to file (less frequent to avoid overhead)\nSAVE_PLOT_INTERVAL = 1000\n\nprint(f\"Total steps: {total_steps}\")\nprint(f\"Training for {NUM_EPOCHS} epochs...\")\nprint(f\"Logging every {EVAL_INTERVAL} steps, saving plot every {SAVE_PLOT_INTERVAL} steps\")\nprint(\"=\"*80)\n\nglobal_step = 0\nbest_val_acc = 0.0\n\nfor epoch in range(NUM_EPOCHS):\n    model.train()\n    \n    for batch in train_loader:\n        loss = compute_loss(model, batch, DEVICE)\n        \n        optimizer.zero_grad()\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), GRAD_CLIP)\n        optimizer.step()\n        scheduler.step()\n        \n        global_step += 1\n        \n        # Evaluate and log (text only)\n        if global_step % EVAL_INTERVAL == 0:\n            train_metrics = evaluate(model, train_loader, DEVICE, max_batches=20)\n            val_metrics = evaluate(model, val_loader, DEVICE)\n            logger.log(global_step, train_metrics, val_metrics)\n            \n            if val_metrics['seq_acc'] > best_val_acc:\n                best_val_acc = val_metrics['seq_acc']\n                torch.save(model.state_dict(), 'best_model.pt')\n        \n        # Save plot to file periodically (not displayed inline)\n        if global_step % SAVE_PLOT_INTERVAL == 0:\n            logger.save_plot()\n\n# Save final plot\nlogger.save_plot()\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"Training complete!\")\nprint(f\"Best validation accuracy: {best_val_acc:.1%}\")\nprint(f\"Final plot saved to: training_curves_depth{TREE_DEPTH}.png\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 6: Final Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Final evaluation\ntrain_metrics = evaluate(model, train_loader, DEVICE)\nval_metrics = evaluate(model, val_loader, DEVICE)\n\nprint(\"=\"*80)\nprint(\"FINAL RESULTS\")\nprint(\"=\"*80)\nprint(f\"Tree Depth: {TREE_DEPTH} ({TREE_DEPTH - 1}-hop reasoning)\")\nprint(f\"Train: Loss={train_metrics['loss']:.4f}, Seq Acc={train_metrics['seq_acc']:.1%}\")\nprint(f\"Val:   Loss={val_metrics['loss']:.4f}, Seq Acc={val_metrics['seq_acc']:.1%}\")\nprint(f\"\\nBest Val Seq Acc: {best_val_acc:.1%}\")\nprint(\"=\"*80)\n\n# Display the final saved plot\nfrom IPython.display import Image, display\nplot_path = f\"training_curves_depth{TREE_DEPTH}.png\"\ntry:\n    display(Image(filename=plot_path))\nexcept:\n    print(f\"Plot saved at: {plot_path}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 7: Test Generation (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on a sample\n",
    "model.eval()\n",
    "sample = val_dataset[0]\n",
    "input_ids = sample['input_ids'].unsqueeze(0).to(DEVICE)\n",
    "\n",
    "# Find [ANSWER] position\n",
    "answer_token_id = tokenizer.token_to_id['[ANSWER]']\n",
    "answer_pos = (input_ids[0] == answer_token_id).nonzero()\n",
    "\n",
    "if len(answer_pos) > 0:\n",
    "    prompt_end = answer_pos[0].item() + 2\n",
    "    prompt = input_ids[:, :prompt_end]\n",
    "    \n",
    "    generated = model.generate(prompt, max_new_tokens=20)\n",
    "    \n",
    "    print(\"=== INPUT ===\")\n",
    "    print(tokenizer.decode(prompt[0].tolist()))\n",
    "    print(\"\\n=== GENERATED ===\")\n",
    "    gen_part = generated[0, prompt_end:].tolist()\n",
    "    print(tokenizer.decode(gen_part))\n",
    "    print(\"\\n=== EXPECTED ===\")\n",
    "    labels = sample['labels'].tolist()\n",
    "    target_ids = [l for l in labels if l != -100 and l != tokenizer.pad_token_id]\n",
    "    print(tokenizer.decode(target_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 8: Download Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Save with depth-specific filename\noutput_filename = f'depth{TREE_DEPTH}_trained.pt'\n\ntorch.save({\n    'model_state_dict': model.state_dict(),\n    'config': model_config,\n    'history': logger.history,\n    'best_val_acc': best_val_acc,\n    'tree_depth': TREE_DEPTH,\n    'hops_required': TREE_DEPTH - 1,\n    'data_format': 'true_induction',\n    'filter_depth_of_truth': FILTER_DEPTH_OF_TRUTH,\n}, output_filename)\n\nprint(f\"Model saved to {output_filename}\")\nprint(f\"  Tree depth: {TREE_DEPTH}\")\nprint(f\"  Hops required: {TREE_DEPTH - 1}\")\nprint(f\"  Best validation accuracy: {best_val_acc:.1%}\")\nprint(f\"  Data format: True Induction (depth_of_truth={FILTER_DEPTH_OF_TRUTH})\")\n\n# Also save just the state dict for easier loading\nstate_dict_filename = f'depth{TREE_DEPTH}_trained_state_dict.pt'\ntorch.save(model.state_dict(), state_dict_filename)\nprint(f\"\\nState dict also saved to {state_dict_filename}\")\n\n# Download (Colab only)\ntry:\n    from google.colab import files\n    files.download(output_filename)\n    print(f\"\\n‚úì {output_filename} downloaded!\")\nexcept:\n    print(f\"\\nNot running in Colab - files saved locally\")"
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "beyond-deduction",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}