============================================================
TESTING QWEN-0.6B ON SYMBOLIC ONTOLOGY REASONING
============================================================

============================================================
DEPTH-2 (1-hop reasoning)
============================================================
Generating 100 test samples...
Generated 100 samples

Sample input (first 200 chars):
  [WORLD_MODEL]
∀x: c2(x) -> p1(x)
∀x: c1(x) -> p3(x)
∀x: c1(x) -> c0(x)
c2(e3)
c1(e0)
c1(e1)
∀x: c2(x) -> c0(x)
c2(e2)
[OBSERVATIONS]
p2(e2)
p2(e3)
c1(e1)
p2(e1)
c2(e3)
c2(e2)
[TASK]
Infer the most gen...

Expected output: ∀x: c0(x) -> p2(x)

Evaluating 100 samples...
  DEBUG raw output: '∀x: c0(x) -> p2(x)'
  DEBUG raw output: '∀x: c0(x) -> p1(x)'
  DEBUG raw output: '∀x: c0(x) -> p3(x)'

Sample predictions:
  [✓] Expected: ∀x: c0(x) -> p2(x)
       Got: ∀x: c0(x) -> p2(x)
  [✓] Expected: ∀x: c0(x) -> p1(x)
       Got: ∀x: c0(x) -> p1(x)
  [✓] Expected: ∀x: c0(x) -> p3(x)
       Got: ∀x: c0(x) -> p3(x)

>>> DEPTH-2 ACCURACY: 93.0% (93/100) <<<

============================================================
DEPTH-3 (2-hop reasoning)
============================================================
Generating 100 test samples...
Generated 100 samples

Sample input (first 200 chars):
  [WORLD_MODEL]
c6(e7)
∀x: c4(x) -> c1(x)
∀x: c2(x) -> p1(x)
c4(e3)
∀x: c4(x) -> p1(x)
∀x: c1(x) -> c0(x)
c3(e0)
∀x: c2(x) -> c0(x)
∀x: c6(x) -> c2(x)
c4(e2)
∀x: c1(x) -> p3(x)
c6(e6)
∀x: c5(x) -> c2(x)...

Expected output: ∀x: c0(x) -> p4(x)

Evaluating 100 samples...

Sample predictions:
  [✓] Expected: ∀x: c0(x) -> p4(x)
       Got: ∀x: c0(x) -> p4(x)
  [✓] Expected: ∀x: c0(x) -> p5(x)
       Got: ∀x: c0(x) -> p5(x)
  [✓] Expected: ∀x: c0(x) -> p3(x)
       Got: ∀x: c0(x) -> p3(x)

>>> DEPTH-3 ACCURACY: 91.0% (91/100) <<<

============================================================
DEPTH-4 (3-hop reasoning)
============================================================
Generating 100 test samples...
Generated 100 samples

Sample input (first 200 chars):
  [WORLD_MODEL]
∀x: c5(x) -> c2(x)
∀x: c3(x) -> c1(x)
∀x: c6(x) -> c2(x)
∀x: c4(x) -> c1(x)
∀x: c10(x) -> c4(x)
∀x: c1(x) -> c0(x)
∀x: c13(x) -> c6(x)
c12(e10)
∀x: c1(x) -> p2(x)
c9(e4)
∀x: c14(x) -> c6...

Expected output: ∀x: c0(x) -> p1(x)

Evaluating 100 samples...

Sample predictions:
  [✓] Expected: ∀x: c0(x) -> p1(x)
       Got: ∀x: c0(x) -> p1(x)
  [✓] Expected: ∀x: c0(x) -> p5(x)
       Got: ∀x: c0(x) -> p5(x)
  [✓] Expected: ∀x: c0(x) -> p2(x)
       Got: ∀x: c0(x) -> p2(x)

>>> DEPTH-4 ACCURACY: 93.0% (93/100) <<<

============================================================
DEPTH-5 (4-hop reasoning)
============================================================
Generating 100 test samples...
Generated 100 samples

Sample input (first 200 chars):
  [WORLD_MODEL]
∀x: c19(x) -> c9(x)
∀x: c18(x) -> p2(x)
c25(e21)
∀x: c17(x) -> c8(x)
c21(e12)
c27(e25)
∀x: c16(x) -> c7(x)
c23(e17)
∀x: c5(x) -> c2(x)
c20(e10)
c15(e1)
c23(e16)
∀x: c8(x) -> c3(x)
∀x: c1...

Expected output: ∀x: c0(x) -> p3(x)

Evaluating 100 samples...

Sample predictions:
  [✓] Expected: ∀x: c0(x) -> p3(x)
       Got: ∀x: c0(x) -> p3(x)
  [✓] Expected: ∀x: c0(x) -> p2(x)
       Got: ∀x: c0(x) -> p2(x)
  [✓] Expected: ∀x: c0(x) -> p1(x)
       Got: ∀x: c0(x) -> p1(x)

>>> DEPTH-5 ACCURACY: 98.0% (98/100) <<<

============================================================
DEPTH-6 (5-hop reasoning)
============================================================
Generating 100 test samples...
Generated 87 samples

Sample input (first 200 chars):
  [WORLD_MODEL]
c48(e35)
c61(e61)
∀x: c16(x) -> c7(x)
∀x: c9(x) -> p3(x)
∀x: c42(x) -> p1(x)
∀x: c59(x) -> c29(x)
∀x: c39(x) -> c19(x)
∀x: c35(x) -> c17(x)
∀x: c57(x) -> c28(x)
∀x: c31(x) -> c15(x)
∀x: ...

Expected output: ∀x: c0(x) -> p4(x)

Evaluating 87 samples...

Sample predictions:
  [✓] Expected: ∀x: c0(x) -> p4(x)
       Got: ∀x: c0(x) -> p4(x)
  [✓] Expected: ∀x: c0(x) -> p2(x)
       Got: ∀x: c0(x) -> p2(x)
  [✓] Expected: ∀x: c0(x) -> p1(x)
       Got: ∀x: c0(x) -> p1(x)

>>> DEPTH-6 ACCURACY: 95.4% (83/87) <<<

============================================================
SUMMARY: QWEN-0.6B ON SYMBOLIC FORMAT
============================================================
Model: qwen-0.6b (596.0M params, context: 32768)

Depth-2 (1-hop): 93.0% [PASS]
Depth-3 (2-hop): 91.0% [PASS]
Depth-4 (3-hop): 93.0% [PASS]
Depth-5 (4-hop): 98.0% [PASS]
Depth-6 (5-hop): 95.4% [PASS]

============================================================
COMPARISON
============================================================

Custom Transformer (1.9M params, trained):
  Depth-2: 100.0%  |  Depth-3: 100.0%  |  Depth-4: 100.0%  |  Depth-5: 82.5%

GPT-2 on Natural Language (few-shot, 124M params):
  Depth-2: 42.0%   |  Depth-3: 16.0%   |  Depth-4: 6.0%    |  Depth-5: 8.0%

GPT-2 on Symbolic Format (few-shot, 124M params):
  Depth-2: 86.0%   |  Depth-3: 64.0%   |  Depth-4: 86.0%   |  Depth-5: 2.0% (context limit)

qwen-0.6b on Symbolic Format (few-shot, 596.0M params):
  Depth-2: 93.0%   |  Depth-3: 91.0%   |  Depth-4: 93.0%   |  Depth-5: 98.0%   |  Depth-6: 95.4%
